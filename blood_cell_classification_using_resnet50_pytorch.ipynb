{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blood_cell_classification_using_resnet50_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg-sgVlK1FlC"
      },
      "source": [
        "# Setup Necessary Things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azHqUTm2QSIJ"
      },
      "source": [
        "Follow instructions on this url to setup kaggle.json in colab session.\n",
        "\n",
        "https://www.kaggle.com/general/74235"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD1BpBt5JaHA"
      },
      "source": [
        "# upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9nkU6DeJshh"
      },
      "source": [
        "# kaggle already available in google colab, uncomment if not using google colab\n",
        "# !pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/blood-cells\n",
        "!unzip -q blood-cells.zip -d blood-cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JbGdwqitIuk"
      },
      "source": [
        "Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt3Txr_MoeFp"
      },
      "source": [
        "# common imports\n",
        "import os, cv2, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "\n",
        "# plotting imports\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
        "RS = 123\n",
        "\n",
        "# data processing imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from umap import UMAP\n",
        "\n",
        "# performance metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import Adam\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# torchvision imports\n",
        "import torchvision.utils as utils #import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# tensorboard imports\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'running on {device}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECGLXeKF2uAu"
      },
      "source": [
        "# Define Custom Network\r\n",
        "\r\n",
        "Adjust ResNet for Blood Cell Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euzw_FmazOIN"
      },
      "source": [
        "class BloodCellResNet(nn.Module):\r\n",
        "    def __init__(self, NUM_CLASSES):\r\n",
        "        super().__init__()\r\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\r\n",
        "        num_ftrs = self.resnet50.fc.in_features\r\n",
        "        self.resnet50.fc = nn.Linear(num_ftrs, 256)\r\n",
        "\r\n",
        "        self.fc2 = nn.Linear(256, 128)\r\n",
        "        self.fc3 = nn.Linear(128, 64)\r\n",
        "        self.fc4 = nn.Linear(64, 32)\r\n",
        "        self.fc5 = nn.Linear(32, NUM_CLASSES)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.resnet50(x))\r\n",
        "        x = F.dropout(x)\r\n",
        "\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = F.dropout(x)\r\n",
        "\r\n",
        "        x = F.relu(self.fc3(x))\r\n",
        "        x = F.dropout(x)\r\n",
        "\r\n",
        "        x = F.relu(self.fc4(x))\r\n",
        "        x = F.dropout(x)\r\n",
        "\r\n",
        "        x = self.fc5(x)\r\n",
        "\r\n",
        "        return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FvPa0Mx28Wb"
      },
      "source": [
        "# Define Custom Dataset\r\n",
        "\r\n",
        "To encapsulate Blood Cell dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjy-IGSazNp7"
      },
      "source": [
        "class BloodCellDataset(Dataset):\r\n",
        "    def __init__(self, path, transform=None):\r\n",
        "        self.images, self.labels = self.get_data(path)\r\n",
        "        self.transform = transform\r\n",
        "        self.classes = ['NEUTROPHIL','EOSINOPHIL', 'MONOCYTE', 'LYMPHOCYTE']\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        image = cv2.resize(cv2.imread(self.images[idx]), (224, 224))\r\n",
        "        label = self.labels[idx]\r\n",
        "        # label = torch.Tensor([0, 0, 0, 0])\r\n",
        "        # label[label_class] = 1\r\n",
        "        if self.transform:\r\n",
        "            image = self.transform(image)\r\n",
        "        return image, label\r\n",
        "    \r\n",
        "    def get_data(self, folder):\r\n",
        "        mappings = dict(zip(['NEUTROPHIL','EOSINOPHIL', 'MONOCYTE', 'LYMPHOCYTE'], list(range(0,4))))\r\n",
        "        images = []\r\n",
        "        labels = []\r\n",
        "        for subtype in os.listdir(folder):\r\n",
        "            if not subtype.startswith('.'):\r\n",
        "                label = mappings[subtype]\r\n",
        "            for image_name in tqdm(os.listdir(os.path.join(folder, subtype))):\r\n",
        "                images.append(os.path.join(folder, subtype, image_name))\r\n",
        "                labels.append(label)\r\n",
        "        return np.asarray(images), np.asarray(labels)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr7vOBtytYEo"
      },
      "source": [
        "Define important variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9dzHswetfG1"
      },
      "source": [
        "# directories path\r\n",
        "DATASET_ROOT = '/content/blood-cells/dataset2-master/dataset2-master/images'\r\n",
        "PATH_TRAIN = os.path.join(DATASET_ROOT, 'TRAIN')\r\n",
        "PATH_VALIDATION = os.path.join(DATASET_ROOT, 'TEST')\r\n",
        "PATH_TEST = os.path.join(DATASET_ROOT, 'TEST_SIMPLE')\r\n",
        "PATH_LOG_DIR = '/content/log_dir_blood_cells/'\r\n",
        "PATH_SAVED_MODELS = '/content/saved_models'\r\n",
        "if os.path.exists(PATH_SAVED_MODELS):\r\n",
        "    os.mkdir(PATH_SAVED_MODELS)\r\n",
        "\r\n",
        "# setting hyper-parameters\r\n",
        "BATCH_SIZE_TRAIN = 8\r\n",
        "BATCH_SIZE_VAL = 16\r\n",
        "BATCH_SIZE_TEST = 16\r\n",
        "lr = 0.0025\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvpNZ-B-tjMB"
      },
      "source": [
        "# Create Dataset and Dataloader Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffJB7Yk4of9-"
      },
      "source": [
        "# create dataset objects\n",
        "dataset_train = BloodCellDataset(path=PATH_TRAIN, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.6606, 0.6414, 0.6787), (0.2602, 0.2627, 0.2635))]))\n",
        "dataset_val = BloodCellDataset(path=PATH_VALIDATION, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "dataset_test = BloodCellDataset(path=PATH_TEST, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "print('\\ndataset_train:', len(dataset_train))\n",
        "print('dataset_val:', len(dataset_val))\n",
        "print('dataset_test:', len(dataset_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp4xx4q8tr7_"
      },
      "source": [
        "# create dataloader objects\r\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\r\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE_VAL, shuffle=False)\r\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False)\r\n",
        "print('dataloader_train:', len(dataloader_train))\r\n",
        "print('dataloader_val:', len(dataloader_val))\r\n",
        "print('dataloader_test:', len(dataloader_test))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyuaBG9lEAEm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpcd4yT_LFrS"
      },
      "source": [
        "def train_model(model, dataloader_train, dataloader_val=None, epochs=1, lr=0.01, debug=False):\n",
        "    TAG = '[train_model]'\n",
        "    # create object for writing to tensorboard\n",
        "    writer = SummaryWriter(PATH_LOG_DIR)\n",
        "    # define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # define optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        y_true, y_pred, y_score = list(), list(), list()\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in tqdm(enumerate(dataloader_train), leave=False, total=len(dataloader_train), position=0):\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.float(), targets.long()\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            if debug:\n",
        "                print('\\n', TAG, '[inputs]', inputs.shape, inputs.dtype)\n",
        "                print('\\n', TAG, '[targets]', targets.shape, targets.dtype)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            yhat = model(inputs)\n",
        "            if debug: print(TAG, '[yhat]', yhat.shape, yhat.dtype)\n",
        "            loss = criterion(yhat, targets)\n",
        "            if debug: print(TAG, '[loss]', loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            yhat = yhat.detach().cpu().numpy()\n",
        "            if debug: print('\\n', TAG, '[yhat]', yhat.shape)\n",
        "            targets = targets.detach().cpu().numpy().reshape((-1, 1))\n",
        "            epoch_loss += loss.item()\n",
        "            y_true.append(targets)\n",
        "            y_pred.append(yhat.argmax(axis=1).reshape((-1, 1)))\n",
        "            y_score.append(yhat.reshape((-1, len(dataloader_train.dataset.classes))))\n",
        "    \n",
        "        y_true, y_pred, y_score = np.vstack(y_true), np.vstack(y_pred), np.vstack(y_score)\n",
        "        epoch_loss = epoch_loss / len(dataloader_train)\n",
        "        metric_accuracy_score = accuracy_score(y_true, y_pred)\n",
        "        metric_f1_score = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "        print(f'\\n[training] | {epoch+1}/{epochs} | loss={epoch_loss:8.6f} | accuracy={metric_accuracy_score:8.6f} | f1_score={metric_f1_score:8.6f}')\n",
        "        # write metric measures to tensorboard\n",
        "        writer.add_scalars('Loss', {'train': epoch_loss}, epoch+1)\n",
        "        writer.add_scalars('Accuracy', {'train': metric_accuracy_score}, epoch+1)\n",
        "        writer.add_scalars('F1_Score', {'train': metric_f1_score}, epoch+1)\n",
        "\n",
        "        if dataloader_val:\n",
        "            validation_loss, metric_accuracy_score, metric_f1_score = evaluate_model(dataloader_val, model)\n",
        "            print(f'\\n[validation] | {epoch+1}/{epochs} | loss={validation_loss:8.6f} | accuracy={metric_accuracy_score:8.6f} | f1_score={metric_f1_score:8.6f}')\n",
        "            # write metric measures to tensorboard\n",
        "            writer.add_scalars('Loss', {'train': validation_loss}, epoch+1)\n",
        "            writer.add_scalars('Accuracy', {'loss': metric_accuracy_score}, epoch+1)\n",
        "            writer.add_scalars('F1_Score', {'loss': metric_f1_score}, epoch+1)\n",
        "        \n",
        "        if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict, f'saved_models/blood_cell_resnet50_lr{lr}_e{epoch}.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zqBvsH8uxjP"
      },
      "source": [
        "Method for performing evaluation on dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j84V9a67LI6z"
      },
      "source": [
        "def evaluate_model(dataloader, model, debug=False):\n",
        "    TAG = '[evaluate_model]'\n",
        "    y_true, y_pred, y_score = list(), list(), list()\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    for i, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader), position=0):\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.float(), targets.long()\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        if debug:\n",
        "            print('\\n', TAG, '[inputs]', inputs.shape, inputs.dtype)\n",
        "            print('\\n', TAG, '[targets]', targets.shape, targets.dtype)\n",
        "\n",
        "        yhat = model(inputs)\n",
        "        if debug: print('\\n', TAG, '[yhat]', yhat.shape, yhat.dtype)\n",
        "        loss = criterion(yhat, targets)\n",
        "        if debug: print('\\n', TAG, '[loss]', loss.item())\n",
        "\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        if debug: print('\\n', TAG, '[yhat]', yhat.shape)\n",
        "        targets = targets.detach().cpu().numpy().reshape((-1, 1))\n",
        "        total_loss += loss.item()\n",
        "        y_true.append(targets)\n",
        "        y_pred.append(yhat.argmax(axis=1).reshape((-1, 1)))\n",
        "        y_score.append(yhat.reshape((-1, len(dataloader.dataset.classes))))\n",
        "    \n",
        "    y_true, y_pred, y_score = np.vstack(y_true), np.vstack(y_pred), np.vstack(y_score)\n",
        "    if debug: print('\\n', TAG, 'y_true.shape:', y_true.shape, 'y_pred.shape:', y_pred.shape, 'y_score.shape:', y_score.shape)\n",
        "    if debug: print('\\n', TAG, np.unique(y_true), np.unique(y_pred))\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    metric_accuracy_score = accuracy_score(y_true, y_pred)\n",
        "    metric_f1_score = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    return (total_loss, metric_accuracy_score, metric_f1_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vwDWTjGogyZ"
      },
      "source": [
        "model = BloodCellResNet(4).to(device)\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D29LKDzXu-UX"
      },
      "source": [
        "To resolve \"memory out of bound\" error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSEyNlb7GwI0"
      },
      "source": [
        "import gc\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDihdwozLNBH"
      },
      "source": [
        "epochs = 10\n",
        "lr = 0.0025\n",
        "train_model(model, dataloader_train, dataloader_val, epochs, lr, debug=False)\n",
        "torch.save(network.state_dict(), f'blood_cell_resnet50_e{epochs}.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eDP-HqdvGIR"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ7weuUvRiN_"
      },
      "source": [
        "# !rm -rf /content/log_dir_blood_cells"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbU-nZ6HXpCz"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/log_dir_blood_cells\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD3J7Jtp750s"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVL-6JzPBNEb"
      },
      "source": [
        "# for loading saved model from .pt file\n",
        "model = BloodCellResNet(4).to(device)\n",
        "model.load_state_dict(torch.load('cell_classification_resnet18.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3kB-02CXPGz"
      },
      "source": [
        "total_loss, metric_accuracy_score, metric_f1_score = evaluate_model(dataloader_test, model, debug=False)\r\n",
        "print(f\"\\n[testing] | accuracy={metric_accuracy_score} | loss={total_loss} | f1_score={metric_f1_score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx0gvL5CyWme"
      },
      "source": [
        "# Confusion Matrix\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3lZ9S_jLL1c"
      },
      "source": [
        "def get_predictions(dataloader, model):\n",
        "    TAG = '[get_predictions]'\n",
        "    y_true, y_pred, y_score = list(), list(), list()\n",
        "    model.eval()\n",
        "    for i, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader), position=0):\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.float(), targets.long()\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        yhat = model(inputs)\n",
        "\n",
        "        yhat = yhat.detach().cpu().numpy()\n",
        "        targets = targets.detach().cpu().numpy().reshape((-1, 1))\n",
        "        y_true.append(targets)\n",
        "        y_pred.append(yhat.argmax(axis=1).reshape((-1, 1)))\n",
        "        y_score.append(yhat.reshape((-1, len(dataloader.dataset.classes))))\n",
        "\n",
        "    return (np.vstack(y_true), np.vstack(y_pred), np.vstack(y_score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMo9YO6PhBwZ"
      },
      "source": [
        "train_true, train_pred, train_score = get_predictions(dataloader_train, model)\r\n",
        "val_true, val_pred, val_score = get_predictions(dataloader_val, model)\r\n",
        "test_true, test_pred, test_score = get_predictions(dataloader_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIGFE_i2iCwR"
      },
      "source": [
        "train_confusion_matrix = confusion_matrix(train_true, train_pred)\r\n",
        "print('\\n[train_confusion_matrix]\\n', train_confusion_matrix)\r\n",
        "val_confusion_matrix = confusion_matrix(val_true, val_pred)\r\n",
        "print('\\n[val_confusion_matrix]\\n', val_confusion_matrix)\r\n",
        "test_confusion_matrix = confusion_matrix(test_true, test_pred)\r\n",
        "print('\\n[test_confusion_matrix]\\n', test_confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGN304CTrZAQ"
      },
      "source": [
        "# Vislualization | PCA, T-SNE, UMAP\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8jlzqyVry3b"
      },
      "source": [
        "# Utility function to visualize the outputs of PCA and t-SNE\r\n",
        "def custom_scatter(x, colors, title):\r\n",
        "    # print('x.shape ->', x.shape)\r\n",
        "    colors = np.array(colors).reshape(-1)\r\n",
        "    # print('colors.shape ->', colors.shape)\r\n",
        "    # choose a color palette with seaborn.\r\n",
        "    num_classes = len(np.unique(colors))\r\n",
        "    # print('num_classes ->', num_classes)\r\n",
        "    palette = np.array(sns.color_palette(\"hls\", num_classes))\r\n",
        "    # print('palette.shape ->', palette.shape)\r\n",
        "\r\n",
        "    # create a scatter plot.\r\n",
        "    f = plt.figure(figsize=(15, 15))\r\n",
        "    f.suptitle(title)\r\n",
        "    ax = plt.subplot(aspect='equal')\r\n",
        "\r\n",
        "    # c = palette[colors.astype(np.int)]\r\n",
        "    # print('c.shape ->', c.shape)\r\n",
        "    # c = np.squeeze(c, axis=(1,))\r\n",
        "\r\n",
        "    # print(x[:,0].shape, x[:,1].shape, palette[colors.astype(np.int)].shape)\r\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\r\n",
        "    # plt.xlim(-25, 25)\r\n",
        "    # plt.ylim(-25, 25)\r\n",
        "    ax.axis('off')\r\n",
        "    ax.axis('tight')\r\n",
        "\r\n",
        "    # add the labels for each digit corresponding to the label\r\n",
        "    txts = []\r\n",
        "\r\n",
        "    for i in range(num_classes):\r\n",
        "        # Position of each label at median of data points.\r\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\r\n",
        "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\r\n",
        "        txt.set_path_effects([PathEffects.Stroke(linewidth=5, foreground=\"w\"), PathEffects.Normal()])\r\n",
        "        txts.append(txt)\r\n",
        "\r\n",
        "    return f, ax, sc, txts\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHGjglceqGBR"
      },
      "source": [
        "mappings = {'EOSINOPHIL': 0, 'LYMPHOCYTE': 1, 'MONOCYTE': 2, 'NEUTROPHIL': 3}\n",
        "root_path = '/content/blood-cells/dataset2-master/dataset2-master/images/TRAIN/'\n",
        "images = []\n",
        "labels = []\n",
        "num_images = 500\n",
        "\n",
        "for folder_name in os.listdir(root_path):\n",
        "    print('folder_name ->', folder_name)\n",
        "    image_label = mappings[folder_name]\n",
        "    folder_path = os.path.join(root_path, folder_name)\n",
        "    for i, image_name in enumerate(os.listdir(folder_path)):\n",
        "        if i >= num_images: break\n",
        "        labels.append(image_label)\n",
        "        image_path = os.path.join(root_path, folder_name, image_name)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = cv2.resize(image, (128, 128))\n",
        "        images.append(image)\n",
        "\n",
        "images = np.array(images).reshape(num_images * 4, -1)\n",
        "labels = np.array(labels)\n",
        "print('images.shape ->', images.shape)\n",
        "print('labels.shape ->', labels.shape)\n",
        "\n",
        "# normalize images\n",
        "print(f'images.mean(): {images.mean():10.6f}, images.std(): {images.std():10.6f}')\n",
        "images = (images - images.mean())\n",
        "print(f'images.mean(): {images.mean():10.6f}, images.std(): {images.std():10.6f}')\n",
        "images = images / images.std()\n",
        "print(f'images.mean(): {images.mean():10.6f}, images.std(): {images.std():10.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwS3JDs4w2-j"
      },
      "source": [
        "Save images in tensorboard summarywriter for PCA, TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZCPqCWrGzLR"
      },
      "source": [
        "# writer = SummaryWriter()\n",
        "# label_images = images.reshape(2000, 1, 128, 128)\n",
        "# print(label_images.shape, label_images.dtype)\n",
        "# writer.add_embedding(images, metadata=labels, label_img=torch.from_numpy(label_images))\n",
        "# writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiktiTxmw8Ij"
      },
      "source": [
        "Manually Create PCA, TSNE using library functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mURE5TmxrcKk"
      },
      "source": [
        "# x_subset = df_test.iloc[:10000, :-1]\r\n",
        "# y_subset = df_test.iloc[:10000, -1]\r\n",
        "images = df_main\r\n",
        "x_subset = df_main.iloc[:, :-1].values\r\n",
        "y_subset = df_main.iloc[:, -1].values\r\n",
        "\r\n",
        "# # PCA Visualization\r\n",
        "time_start = time.time()\r\n",
        "components = 10\r\n",
        "pca = PCA(n_components=components)\r\n",
        "pca_result = pca.fit_transform(x_subset)\r\n",
        "print(pca_result.shape)\r\n",
        "print('PCA done! Time elapsed: {} seconds'.format(time.time() - time_start))\r\n",
        "\r\n",
        "pca_df = pd.DataFrame(columns=[f'pca{i+1}' for i in range(components)])\r\n",
        "for i in range(components):\r\n",
        "    pca_df['pca' + str(i + 1)] = pca_result[:, i]\r\n",
        "\r\n",
        "variances = pca.explained_variance_ratio_[:components]\r\n",
        "print('Variance explained per principal component:\\n{}'.format(variances))\r\n",
        "\r\n",
        "# create a scree plot\r\n",
        "f = plt.figure(figsize=(12, 12))\r\n",
        "ax = plt.subplot(aspect='equal')\r\n",
        "ax.bar(range(components), variances)\r\n",
        "ax.scatter(range(components), variances)\r\n",
        "ax.axis('tight')\r\n",
        "\r\n",
        "two_comp = pca_df[['pca1','pca2']] # taking first and second principal component\r\n",
        "print('# Visualizing the PCA output, components = (pca1, pca2)')\r\n",
        "f, ax, sc, txts = custom_scatter(two_comp.values, y_subset)\r\n",
        "print(f, ax, sc, txts, sep='\\n') # Visualizing the PCA output\r\n",
        "\r\n",
        "# two_comp = pca_df[['pca1','pca3']] # taking first and second principal component\r\n",
        "# print('# Visualizing the PCA output, components = (pca1, pca3)')\r\n",
        "# f, ax, sc, txts = custom_scatter(two_comp.values, y_subset)\r\n",
        "# print(f, ax, sc, txts, sep='\\n') # Visualizing the PCA output\r\n",
        "\r\n",
        "# two_comp = pca_df[['pca2','pca3']] # taking first and second principal component\r\n",
        "# print('# Visualizing the PCA output, components = (pca2, pca3)')\r\n",
        "# f, ax, sc, txts = custom_scatter(two_comp.values, y_subset)\r\n",
        "# print(f, ax, sc, txts, sep='\\n') # Visualizing the PCA output\r\n",
        "\r\n",
        "# # t-SNE Visualization\r\n",
        "# time_start = time.time()\r\n",
        "# tsne = TSNE(random_state=RS).fit_transform(x_subset)\r\n",
        "# print('t-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\r\n",
        "# print('# Visualizing the t-SNE output')\r\n",
        "# custom_scatter(tsne, y_subset)\r\n",
        "\r\n",
        "# # Recommended Approach, first PCA then t-SNE\r\n",
        "# time_start = time.time()\r\n",
        "# pca_10 = PCA(n_components=10)\r\n",
        "# pca_result_10 = pca_10.fit_transform(x_subset)\r\n",
        "# print('PCA with 10 components done! Time elapsed: {} seconds'.format(time.time()-time_start))\r\n",
        "# print('Cumulative variance explained by 10 principal components: {}'.format(np.sum(pca_10.explained_variance_ratio_)))\r\n",
        "# time_start = time.time()\r\n",
        "# pca_then_tsne = TSNE(random_state=RS).fit_transform(pca_result_10)\r\n",
        "# print('t-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start))\r\n",
        "# print('# Visualizing the PCA then t-SNE output')\r\n",
        "# custom_scatter(pca_then_tsne, y_subset)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuRioEsCzUhL"
      },
      "source": [
        "## UMAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjwIiKwlP6xH"
      },
      "source": [
        "points = 8000\r\n",
        "components = 2\r\n",
        "distances = ['hamming', 'dice', 'jaccard', 'russellrao', 'kulsinski', 'rogerstanimoto', 'sokalmichener', 'sokalsneath', 'yule']\r\n",
        "umap_dfs = {}\r\n",
        "\r\n",
        "for distance in distances:\r\n",
        "    umap_df = pd.DataFrame(columns=[f'umap{i+1}' for i in range(components)])\r\n",
        "    idx = 0\r\n",
        "    while len(umap_df) < len(df_new_features):\r\n",
        "        start = idx * points\r\n",
        "        end = start + points\r\n",
        "        if end > len(df_new_features):\r\n",
        "            end = len(df_new_features)\r\n",
        "        df_main = df_new_features.iloc[start:end]\r\n",
        "\r\n",
        "        x_subset = df_main.iloc[:, :-1].values\r\n",
        "        y_subset = df_main.iloc[:, -1].values\r\n",
        "\r\n",
        "        time_start = time.time()\r\n",
        "        umap1 = UMAP(metric=distance, n_components=components, n_neighbors=25)\r\n",
        "        umap_result = umap1.fit_transform(x_subset)\r\n",
        "        \r\n",
        "        print(umap_result.shape)\r\n",
        "        print('UMAP done! Time elapsed: {} seconds'.format(time.time() - time_start))\r\n",
        "        \r\n",
        "        umap_result_df = pd.DataFrame(columns=[f'umap{i+1}' for i in range(components)])\r\n",
        "        for i in range(components):\r\n",
        "            umap_result_df['umap' + str(i + 1)] = umap_result[:, i]\r\n",
        "        \r\n",
        "        umap_df = pd.concat([umap_df, umap_result_df])\r\n",
        "        idx += 1\r\n",
        "\r\n",
        "    umap_dfs[distance] = umap_df\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PcAP6MJz9SF"
      },
      "source": [
        "for distance in distances:\r\n",
        "    two_comp = umap_dfs[distance][['umap1', 'umap2']] # taking first and second principal component\r\n",
        "    # print('# Visualizing the UMAP output, components = (umap1, umap2)')\r\n",
        "    f, ax, sc, txts = custom_scatter(two_comp.values[:], df_new_features.iloc[:two_comp.values.shape[0], -1], distance)\r\n",
        "    # print(f, ax, sc, txts, sep='\\n') # Visualizing the UMAP output\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}